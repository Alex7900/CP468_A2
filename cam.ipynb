{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100.   -1.   -2.   -1. -100. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100. -100. -100. -100. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2.  100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100.   -1. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -2. -100.   -2.   -1.   -2. -100. -100.]\n",
      "[  -1. -100.   -1. -100.   -1. -100.   -1. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2. -100.   -1. -100.   -1.]\n",
      "[  -1. -100.   -1. -100.   -1. -100.   -1. -100.   -1.]\n",
      "[-100.   -1.   -2.   -1.   -2.   -1.   -2.   -1.   -2.]\n",
      "[-100. -100.   -1. -100. -100. -100.   -1. -100. -100.]\n",
      "[-100. -100. -100.   -1.   -1.   -1.   -2. -100. -100.]\n",
      "Training complete!\n",
      "[[3, 0], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5]]\n",
      "[-100.   -1.   -2.   -1. -100. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100. -100. -100. -100. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2.  100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100.   -1. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -2. -100.   -2.   -1.   -2. -100. -100.]\n",
      "[ 100. -100.   -1. -100.   -1. -100.   -1. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2. -100.   -1. -100.   -1.]\n",
      "[  -1. -100.   -1. -100.   -1. -100.   -1. -100.   -1.]\n",
      "[-100.   -1.   -2.   -1.   -2.   -1.   -2.   -1.   -2.]\n",
      "[-100. -100.   -1. -100. -100. -100.   -1. -100. -100.]\n",
      "[-100. -100. -100.   -1.   -1.   -1.   -2. -100. -100.]\n",
      "Training complete!\n",
      "[[10, 3], [10, 4], [10, 5], [10, 6], [9, 6], [8, 6], [8, 5], [8, 4], [7, 4], [6, 4], [6, 3], [6, 2], [6, 1], [6, 0], [5, 0]]\n",
      "[-100.   -1.   -2.   -1. -100. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100. -100. -100. -100. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2.  100. -100. -100. -100.]\n",
      "[  -1. -100.   -1. -100.   -1. -100. -100. -100. -100.]\n",
      "[  -1. -100.   -2. -100.   -2.   -1.   -2. -100. -100.]\n",
      "[ 100. -100.   -1. -100.   -1. -100.   -1. -100. -100.]\n",
      "[  -2.   -1.   -2.   -1.   -2. -100.   -1. -100.   -1.]\n",
      "[  -1. -100.   -1. -100.   -1. -100.   -1. -100.   -1.]\n",
      "[-100.  100.   -2.   -1.   -2.   -1.   -2.   -1.   -2.]\n",
      "[-100. -100.   -1. -100. -100. -100.   -1. -100. -100.]\n",
      "[-100. -100. -100.   -1.   -1.   -1.   -2. -100. -100.]\n",
      "Training complete!\n",
      "[[6, 8], [7, 8], [8, 8], [8, 7], [8, 6], [8, 5], [8, 4], [8, 3], [8, 2], [8, 1]]\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#define the shape of the environment (i.e its states)\n",
    "rows = 11\n",
    "cols = 9\n",
    "q_values = np.zeros((rows, cols, 4))\n",
    "\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "\n",
    "#Create a 2D numpy array to hold the rewards for each state. \n",
    "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
    "rewards = np.full((rows, cols), -100.)\n",
    "\n",
    "\n",
    "#define road node locations\n",
    "roads = {} \n",
    "roads[0] = [1, 2, 3]\n",
    "roads[1] = [0, 2]\n",
    "roads[2] = [1, 3, 5]\n",
    "roads[3] = [0, 2, 4]\n",
    "roads[4] = [0, 5]\n",
    "roads[5] = [0, 2, 4, 6]\n",
    "roads[6] = [1, 3, 6, 8]\n",
    "roads[7] = [0, 2, 4, 6, 8]\n",
    "roads[8] = [1, 3, 5, 7]\n",
    "roads[9] = [2, 6]\n",
    "roads[10] = [3, 4, 5]\n",
    "\n",
    "\n",
    "#define intersection node locations\n",
    "intersections = {} \n",
    "intersections[0] = [2]\n",
    "intersections[1] = []\n",
    "intersections[2] = [0, 2, 4]\n",
    "intersections[3] = []\n",
    "intersections[4] = [2, 4, 6]\n",
    "intersections[5] = []\n",
    "intersections[6] = [0, 2, 4]\n",
    "intersections[7] = []\n",
    "intersections[8] = [2, 4, 6, 8]\n",
    "intersections[9] = []\n",
    "intersections[10] = [6]\n",
    "\n",
    "\n",
    "#set the rewards for all roads and intersections\n",
    "for row_index in range(0, 11):\n",
    "    for column_index in roads[row_index]:\n",
    "        rewards[row_index, column_index] = -1.\n",
    "for row_index in range(0, 11):\n",
    "    for column_index in intersections[row_index]:\n",
    "        rewards[row_index, column_index] = -2.\n",
    "    \n",
    "\n",
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    #if the reward for this location is -1 or -2 its a valid node (i.e. not a grey square)\n",
    "    if rewards[current_row_index, current_column_index] == -1. or rewards[current_row_index, current_column_index] == -2.:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def invalid_end(row, col):\n",
    "    if rewards[row, col] != -1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "#define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "    #get a random row and column index\n",
    "    current_row_index = np.random.randint(rows)\n",
    "    current_column_index = np.random.randint(cols)\n",
    "    #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(rows)\n",
    "        current_column_index = np.random.randint(cols)\n",
    "    return current_row_index, current_column_index\n",
    "\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "    #then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else: #choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < cols - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "\n",
    "#Define a function that will get the shortest path between any location \n",
    "def get_shortest_path(start_row_index, start_column_index, end_row_index, end_column_index):\n",
    "    #return immediately if this is an invalid starting location\n",
    "    if is_terminal_state(start_row_index, start_column_index) and invalid_end(end_row_index, end_column_index):\n",
    "        return []\n",
    "    else: #if this is a 'legal' starting and ending location\n",
    "        rewards[end_row_index, end_column_index] = 100.\n",
    "        #print rewards matrix\n",
    "        for row in rewards:\n",
    "            print(row)\n",
    "        train()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "        #get the best action to take\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "        #move to the next location on the path, and add the new location to the list\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path\n",
    "\n",
    "\n",
    "def train():\n",
    "    #define training parameters\n",
    "    epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "    discount_factor = 0.9 #discount factor for future rewards\n",
    "    learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "    #run through 1000 training episodes\n",
    "    for episode in range(10000):\n",
    "        #get the starting location for this episode\n",
    "        row_index, column_index = get_starting_location()\n",
    "        #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "        #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "        while not is_terminal_state(row_index, column_index):\n",
    "            #choose which action to take (i.e., where to move next)\n",
    "            action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "            #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "            old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "            row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "            \n",
    "            #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "            reward = rewards[row_index, column_index]\n",
    "            old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "            temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "            #update the Q-value for the previous state and action pair\n",
    "            new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "            q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "    print('Training complete!')\n",
    "\n",
    "\n",
    "#display a few shortest paths\n",
    "print(get_shortest_path(3, 0, 2, 5))\n",
    "print(get_shortest_path(10, 3, 5, 0)) \n",
    "print(get_shortest_path(6, 8, 8, 1))\n",
    "\n",
    "\n",
    "# new_Q_values = []\n",
    "# for i in range(len(q_vals)):\n",
    "#     if i % 25 == 0:\n",
    "#         new_Q_values.append(q_vals[i])\n",
    "\n",
    "# Plotting the convergence of rewards over episodes\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(new_Q_values)\n",
    "# plt.title('Convergence of Q-learning over iterations')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Total reward per iteration')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
